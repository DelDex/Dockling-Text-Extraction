Bootstrap: docker
From: python:3.11-slim

%labels
    Author "Deljin Davis Kanukadan"
    Version "2.0-HPC-Optimized"
    Description "HPC-optimized container for hybrid_chunking.py with full Docling features (TableFormer, OCR)"

%environment
    # Make Python output unbuffered and set model caches
    export PYTHONUNBUFFERED=1
    export HF_HOME=/opt/models
    export TRANSFORMERS_CACHE=/opt/models
    export TORCH_HOME=/opt/models
    export PIP_NO_CACHE_DIR=1
    export PATH=/usr/local/bin:$PATH

    # Docling/OCR specific settings
    export TESSDATA_PREFIX=/usr/share/tesseract-ocr/5/tessdata
    export OMP_NUM_THREADS=4

%files
    # Copy files BEFORE %post section
    hybrid_chunking.py /app/hybrid_chunking.py
    hybrid_chunking_lightweight.py /app/hybrid_chunking_lightweight.py
    requirements.txt /app/requirements.txt
    README.md /app/README.md

%post
    set -e

    echo "=========================================="
    echo "Installing system dependencies for HPC..."
    echo "=========================================="

    apt-get update && apt-get install -y --no-install-recommends \
        git \
        curl \
        ca-certificates \
        ffmpeg \
        poppler-utils \
        tesseract-ocr \
        tesseract-ocr-eng \
        libtesseract-dev \
        libpoppler-cpp-dev \
        && rm -rf /var/lib/apt/lists/*

    echo "System dependencies installed successfully"

    # Create app and cache directories
    mkdir -p /app /opt/models
    chmod -R 777 /opt/models  # Allow writes for model caching

    echo "=========================================="
    echo "Installing Python dependencies..."
    echo "=========================================="

    # Upgrade pip and wheel for better binary installs
    python -m pip install --upgrade pip wheel setuptools

    # Install project Python dependencies from requirements.txt
    pip install -r /app/requirements.txt

    # Install CPU-only PyTorch and torchvision (Windows DLL issues with 2.9.0)
    pip install --index-url https://download.pytorch.org/whl/cpu \
        torch==2.5.0+cpu torchvision==0.20.0+cpu

    echo "Python dependencies installed successfully"

    echo "=========================================="
    echo "Pre-caching models for HPC (requires internet)..."
    echo "=========================================="

    # Pre-cache all models to avoid runtime downloads on HPC
    python - << 'PYSCRIPT'
import os
os.environ['HF_HOME'] = '/opt/models'
os.environ['TRANSFORMERS_CACHE'] = '/opt/models'

print("\n[1/3] Caching tokenizer...")
from transformers import AutoTokenizer
model_id = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
print(f"✓ Cached tokenizer: {model_id}")

print("\n[2/3] Caching Docling models (TableFormer, LayoutLM)...")
from docling.document_converter import DocumentConverter
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.base_models import InputFormat
from docling.document_converter import PdfFormatOption

# Initialize with full pipeline to download all models
pipeline_options = PdfPipelineOptions()
pipeline_options.do_table_structure = True
pipeline_options.do_ocr = True
pipeline_options.generate_page_images = True
pipeline_options.generate_picture_images = True

converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)
print("✓ Cached Docling models (TableFormer, LayoutLM, OCR)")

print("\n[3/3] Verifying installation...")
import torch
import transformers
import docling
print(f"✓ PyTorch version: {torch.__version__}")
print(f"✓ Transformers version: {transformers.__version__}")
print(f"✓ Docling imported successfully")

print("\n========================================")
print("All models cached successfully!")
print("========================================")
PYSCRIPT

    echo "Model pre-caching complete"

    # Set proper permissions
    chmod -R 755 /app

    echo "=========================================="
    echo "Container build complete!"
    echo "=========================================="

%runscript
    # Default entrypoint runs the hybrid chunking script
    cd /app
    echo "=============================================="
    echo "HPC-Optimized Docling Hybrid Chunking"
    echo "Version: 2.0 (Full Features Enabled)"
    echo "=============================================="
    echo "Configuration:"
    echo "  - Table extraction: ENABLED"
    echo "  - OCR: ENABLED"
    echo "  - High-resolution processing: ENABLED"
    echo "=============================================="
    exec python /app/hybrid_chunking.py "$@"

%help
    ======================================================================
    HPC-Optimized Docling Hybrid Chunking Container
    ======================================================================

    This container runs hybrid_chunking.py with FULL Docling features:
    ✓ TableFormer for table extraction
    ✓ OCR for scanned documents
    ✓ High-resolution image processing (2.0x scale)
    ✓ Page and picture image generation

    REQUIREMENTS:
    - 16GB RAM per job (recommended for HPC)
    - Internet access during BUILD (to download models)
    - No internet needed during RUN (models are pre-cached)

    ======================================================================
    BUILD INSTRUCTIONS (on machine with internet)
    ======================================================================

    Using Singularity:
      singularity build --fakeroot hybrid_chunking.sif singularity.def

    Using Apptainer (newer HPCs):
      apptainer build hybrid_chunking.sif singularity.def

    Build time: ~10-15 minutes (downloads ~2GB of models)

    ======================================================================
    RUNNING ON HPC (Interactive)
    ======================================================================

    Basic run (bind documents and outputs):
      singularity run \
        -B $PWD/documents:/app/documents \
        -B $PWD/outputs:/app/outputs \
        hybrid_chunking.sif

    With custom cache location (if /opt/models is read-only):
      singularity run \
        -B $PWD/documents:/app/documents \
        -B $PWD/outputs:/app/outputs \
        -B $PWD/.cache:/opt/models \
        hybrid_chunking.sif

    ======================================================================
    SLURM JOB SCRIPT EXAMPLE
    ======================================================================

    Create a file named 'run_chunking.slurm':

    #!/bin/bash
    #SBATCH --job-name=docling_chunking
    #SBATCH --output=chunking_%j.log
    #SBATCH --error=chunking_%j.err
    #SBATCH --time=02:00:00
    #SBATCH --mem=16G
    #SBATCH --cpus-per-task=4
    #SBATCH --nodes=1

    # Load Singularity/Apptainer module (adjust for your HPC)
    module load singularity

    # Set working directory
    cd $SLURM_SUBMIT_DIR

    # Run container
    singularity run \
      -B $PWD/documents:/app/documents \
      -B $PWD/outputs:/app/outputs \
      hybrid_chunking.sif

    echo "Job completed: $(date)"

    Submit with:
      sbatch run_chunking.slurm

    ======================================================================
    PARALLEL PROCESSING (Multiple Documents)
    ======================================================================

    For large document batches, use SLURM job arrays:

    #!/bin/bash
    #SBATCH --job-name=docling_array
    #SBATCH --output=chunking_%A_%a.log
    #SBATCH --error=chunking_%A_%a.err
    #SBATCH --array=1-10%4        # Process 10 docs, 4 at a time
    #SBATCH --time=01:00:00
    #SBATCH --mem=16G
    #SBATCH --cpus-per-task=4

    module load singularity

    # Get list of PDF files
    FILES=(documents/knowledge/*.pdf)
    FILE=${FILES[$SLURM_ARRAY_TASK_ID-1]}

    echo "Processing: $FILE"

    # Process single file (modify script to accept file argument)
    singularity exec \
      -B $PWD/documents:/app/documents \
      -B $PWD/outputs:/app/outputs \
      hybrid_chunking.sif \
      python /app/hybrid_chunking.py "$FILE"

    ======================================================================
    PERFORMANCE TUNING
    ======================================================================

    Memory allocation guidelines:
    - Simple PDFs (text only): 8GB RAM
    - PDFs with tables: 12GB RAM
    - PDFs with OCR/images: 16GB RAM
    - Large technical docs: 20GB RAM

    CPU threads (set via OMP_NUM_THREADS):
    - Default: 4 threads
    - Adjust in job script: export OMP_NUM_THREADS=8

    ======================================================================
    TROUBLESHOOTING
    ======================================================================

    Issue: "Out of memory" errors
    Solution: Increase --mem in SLURM script (try 20G or 24G)

    Issue: "Model not found" errors
    Solution: Verify models cached during build:
      singularity exec hybrid_chunking.sif ls -lh /opt/models

    Issue: "Permission denied" on /opt/models
    Solution: Bind custom cache directory:
      -B $PWD/.cache:/opt/models

    Issue: Slow processing
    Solution: Increase --cpus-per-task and set OMP_NUM_THREADS

    ======================================================================

    For more information, see README.md and CLAUDE.md in the repository.
